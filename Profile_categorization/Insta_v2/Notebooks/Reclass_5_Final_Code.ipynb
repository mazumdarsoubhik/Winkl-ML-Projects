{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all packages.\n",
      "Loading GoogleNews...\n",
      "Loaded GoogleNews!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from itertools import chain \n",
    "import math\n",
    "import time\n",
    "import logging \n",
    "import requests\n",
    "import json\n",
    "print(\"Imported all packages.\")\n",
    "tic = time.time()\n",
    "print(\"Loading GoogleNews...\")\n",
    "from gensim import models\n",
    "w = models.KeyedVectors.load_word2vec_format(r\"F:\\Pretrained Models\\GoogleNews-vectors-negative300.bin.gz\", binary=True, limit=21000)\n",
    "print(\"Loaded GoogleNews!\")\n",
    "\n",
    "#Frame pre-processing function\n",
    "def process(array,avoidwords):\n",
    "    hashes = len(re.findall(r'#',str(array))) #counting hashtags\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',str(array))  #Remove Numbers\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text) # Remove nums\n",
    "    text = re.sub(r'\\s+',' ',text)  #Remove extra space\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\",' ',text)  #Remove special characters\n",
    "    text = text.lower()  #Lower case all\n",
    "    text = nltk.sent_tokenize(text)  #Tokenize to sentences \n",
    "    keywords = [nltk.word_tokenize(sentence) for sentence in text]\n",
    "    raw_cap = len(keywords[0]) # Total number of words in caption\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(avoidwords)\n",
    "    for i in range(len(keywords)):\n",
    "        keywords[i] = [word for word in keywords[i] if word not in stop_words]\n",
    "    genuinity_percent = (raw_cap-hashes)*100/raw_cap\n",
    "    return keywords,genuinity_percent\n",
    "\n",
    "# normalize() -> given an array, converts to 1/0, top int(pos) will be 1\n",
    "def normalize(keys, pos =3):  \n",
    "    ax = [i for i in keys]\n",
    "    temp = [i for i in keys]\n",
    "    temp.sort()\n",
    "    temp = temp[-pos:]\n",
    "    for x in temp:\n",
    "        ax[keys.index(x)] = 1\n",
    "    for x in range(len(ax)):\n",
    "        if ax[x] != 1:\n",
    "            ax[x] = 0\n",
    "    return ax\n",
    "\n",
    "def normalizeSD(keys, thre =3):    # Given score array return shortlisted cats in given threshold\n",
    "    if sum(keys) == 0:\n",
    "        return keys\n",
    "    ax = deviation(keys)\n",
    "    ax = dev_shortlist(ax,thre)\n",
    "    return ax\n",
    "def deviation(array):\n",
    "    mu = max(array)\n",
    "    l = len(array)\n",
    "    ar = []\n",
    "    for x in range(l):\n",
    "        ar.append(math.sqrt((array[x]-mu)**2)/l)\n",
    "    total = sum(ar)\n",
    "    for x in range(l):\n",
    "        if total != 0:\n",
    "            ar[x] = (ar[x]/total)*100\n",
    "    return ar\n",
    "\n",
    "def mean_deviation(array):\n",
    "    l = len(array)\n",
    "    mu = sum(array)/l\n",
    "    ar = []\n",
    "    for x in range(l):\n",
    "        ar.append(math.sqrt((array[x]-mu)**2)/l)\n",
    "    total = sum(ar)\n",
    "    for x in range(l):\n",
    "        if total != 0:\n",
    "            ar[x] = (ar[x]/total)*100\n",
    "    return ar\n",
    "\n",
    "def dev_shortlist(dev_array,thre = 2):  # Shortlist using threshold from deviation array | return array in 1/0\n",
    "    final_cat = [0]*len(dev_array)\n",
    "    for i in range(len(dev_array)):\n",
    "        if dev_array[i] <=thre:\n",
    "            final_cat[i] = 1\n",
    "    return final_cat\n",
    "\n",
    "# compute() => category[] to be called outside\n",
    "def compute2(caption,category_list,category,top =3):\n",
    "    ar = []\n",
    "    score = []\n",
    "\n",
    "    # Code to get frequency distribution and unique keywords array\n",
    "    keywords = []\n",
    "    caption_freq = []\n",
    "    counts = Counter(caption)\n",
    "    if len(counts) > 0:\n",
    "        labels, values = zip(*counts.items())\n",
    "        ## sort your values in descending order\n",
    "        indSort = np.argsort(values)[::-1]\n",
    "        ## rearrange your data\n",
    "        keywords = np.array(labels)[indSort]  # Label\n",
    "        caption_freq = np.array(values)[indSort]  # Values\n",
    "    \n",
    "    # Detect words not in Google Dict | Put freq = 0\n",
    "    for x in keywords:\n",
    "        try:\n",
    "            restConst = w.similarity(x,'something')\n",
    "        except KeyError:\n",
    "            caption_freq[np.where(keywords == x)] = 0\n",
    "        \n",
    "    #Google similaity function\n",
    "    for c_tag in range(len(category_list)):\n",
    "        empty1 = []\n",
    "        for the_word in keywords:\n",
    "            empty2 = []\n",
    "            for k_tag in range(len(category_list[c_tag])):\n",
    "                try:\n",
    "                    empty2.append(w.similarity(category_list[c_tag][k_tag],the_word))\n",
    "                except:\n",
    "                    empty2.append(0)\n",
    "            empty1.append(max(empty2))\n",
    "        ar.append(empty1)\n",
    "    # Store the similarity values in dataframe\n",
    "    frame = pd.DataFrame()\n",
    "    frame = pd.DataFrame(ar, columns = keywords)\n",
    "  \n",
    "    #Normalize | top select\n",
    "    for key in frame.columns:\n",
    "        frame[key] = normalizeSD(frame[key].tolist(),top)\n",
    "    \n",
    "    # Multiply with frequency\n",
    "    for row in range(len(frame)):\n",
    "        frame.values[row] = [i*j for i,j in zip(frame.values[row],caption_freq)]\n",
    "    # Sum the values => Score\n",
    "    for row in range(len(frame)):\n",
    "        score.append(sum(frame.values[row]))\n",
    "    \n",
    "    frame['category'] = category\n",
    "    frame['Scores'] = score\n",
    "    return frame,keywords[:20]\n",
    "\n",
    "def get_row_pscore(col_name,f1,i,f2,genuinity_score,top_keywords, scoreType):  # f1-mainframe | f2-frame\n",
    "    ud = f1.loc[i,'id']\n",
    "    ul = f1.loc[i,'url']\n",
    "    row_in_array = [ud,ul]\n",
    "    score_array = f2[scoreType].tolist()\n",
    "    row_in_array.extend(score_array)\n",
    "    row_in_array.append(genuinity_score)\n",
    "    row_in_array.append(','.join(map(str,top_keywords)))\n",
    "    zip_it = zip(col_name,row_in_array)\n",
    "    convert_to_dict = dict(zip_it)\n",
    "    return convert_to_dict\n",
    "\n",
    "# To make data in \n",
    "# DB format | API post \n",
    "def to_dict_api(percentages,categories,top_keywords,user_id,g_score): #frame and i to get id\n",
    "    mydict = {}\n",
    "    cat_array =[]\n",
    "    empty_percent = [0]*(len(categories)-len(percentages))\n",
    "    percent_array = [y for y in percentages]\n",
    "    percent_array.extend(empty_percent)\n",
    "    mydict['user_id'] = user_id\n",
    "    mydict['keywords'] = json.dumps(top_keywords.tolist())\n",
    "    for j in range(len(categories)):\n",
    "        cat_array.append({'tag':categories[j],'percentage':percent_array[j]})\n",
    "    mydict['categories'] = json.dumps(cat_array)\n",
    "    mydict['genuinity_score'] = g_score\n",
    "    return mydict \n",
    "\n",
    "\n",
    "# Extended category list\n",
    "categories = ['food', 'fashion', 'makeup', 'beauty', 'lifestyle','luxury', 'travel','photography','fitness','sports','gaming', 'entertainment', 'technology','investment','education', 'animal', 'health', 'inspiration','art','parenting','book']\n",
    "categories_list = [['food', 'recipe', 'cooking'],\n",
    "              ['fashion', 'outfit', 'clothes'],\n",
    "              ['makeup','shades','haircare','face'],\n",
    "              ['beauty','skin', 'oil', 'hair'],\n",
    "              ['lifestyle','style'],\n",
    "              ['luxury','rich','billionaire','car'],\n",
    "              ['travel', 'world', 'destination', 'adventures', 'landscapes','bucket'],\n",
    "              ['photography', 'photo', 'editing'],\n",
    "              ['fitness', 'nutrition', 'workout', 'healthy', 'exercise','run'],\n",
    "              ['sport', 'sports','win','loss'],\n",
    "              ['gaming','stream','freefire','pubg'],\n",
    "              ['entertainment', 'movies', 'series', 'film', 'comedy','actor','actress'],\n",
    "              ['technology', 'tech', 'geek', 'smartphones', 'mobiles'],\n",
    "              ['investment', 'financial', 'stocks', 'market', 'trade'],\n",
    "              ['education', 'lectures', 'competitive', 'exams', 'coaching'],\n",
    "              ['animal', 'wild', 'wildlife', 'nature'],\n",
    "              ['health', 'medical', 'prevention','cure', 'treatment', 'heal'],\n",
    "              ['psychology', 'motivation', 'inspire', 'happiness', 'mind','spiritual'],\n",
    "              ['sketch','DIY','painting','art','drawing'],\n",
    "              ['children','kids','infant','family','parenting','toys'],\n",
    "              ['books','review','journaling','stationery','study']]\n",
    "col_name = ['user_id','url','Food','Fashion', 'Make-up', 'Beauty', 'Lifestyle','Luxury', 'Travel','Photography','Fitness','Sports','Gaming', 'Entertainment', 'Gadgets & Tech','Finance','Education', 'Animal/Pet', 'Health', 'Self Improvement','Art', 'Parenting', 'Books', 'genuinity_score','top keywords']\n",
    "API_categories = ['Food','Fashion', 'Make-up', 'Beauty', 'Lifestyle','Luxury', 'Travel','Photography','Fitness','Sports','Gaming', 'Entertainment', 'Gadgets & Tech','Finance','Education', 'Animal/Pet', 'Health','Self Improvement','Art', 'Parenting', 'Books']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID no. 127 Done! Total 1 ids done\n",
      "ID no. 128 Done! Total 2 ids done\n",
      "ID no. 132 Done! Total 3 ids done\n",
      "ID no. 134 Done! Total 4 ids done\n",
      "ID no. 135 Done! Total 5 ids done\n",
      "ID no. 136 Done! Total 6 ids done\n",
      "ID no. 138 Done! Total 7 ids done\n",
      "ID no. 141 Done! Total 8 ids done\n",
      "ID no. 142 Done! Total 9 ids done\n",
      "ID no. 147 Done! Total 10 ids done\n",
      "Done 1 pages, the last_id is 147 and time taken 34.9656023979187 seconds\n"
     ]
    }
   ],
   "source": [
    "########################## MAIN CODE ###############################\n",
    "\n",
    "x = requests.get('http://44.229.68.155/insta_users/get_uncategorized_accounts?limit=10&current_id=0', headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "status = x.status_code\n",
    "data = x.json()\n",
    "df = pd.DataFrame(data['users'])\n",
    "pages = 0\n",
    "idsdone = 0\n",
    "txt = \"Done {} pages, the last_id is {} and time taken {} seconds\"\n",
    "profile_percentages =  pd.DataFrame(columns = col_name)\n",
    "profile_percentages.to_csv(r'profile_backup.csv',index=False)\n",
    "\n",
    "while(len(data['users']) !=0 and pages <1):\n",
    "    try:\n",
    "        new_tic = time.time()\n",
    "        if(status != 200):\n",
    "            raise Exception(\"GET request error: {}\".format(status))\n",
    "        dfnew = pd.DataFrame(columns=['id','handle','name','url','gender','country','captions','bio'], data = df[['id','handle','name','url','gender','country','captions','bio']].values)\n",
    "        last_id = dfnew['id'].iloc[-1]\n",
    "        # Fresh dataframe\n",
    "        profile_percentages =  pd.DataFrame(columns = col_name)\n",
    "\n",
    "        # START Categorization By Page #\n",
    "        for i in range(len(dfnew)):\n",
    "\n",
    "            try:\n",
    "                #Store userid | caption | total posts\n",
    "                userid = dfnew['id'].iloc[i]\n",
    "                captions = dfnew['captions'].iloc[i]\n",
    "                bio = dfnew['bio'].iloc[i]\n",
    "                total_posts = len(captions)\n",
    "\n",
    "                # Words which mostly occurs in insta post and we want to avoid considering them for the sake of accuracy of results\n",
    "                avoidwords = ['verified','none','follow','like','reposted','influencer','gmail','com','collabs','collaboration','hello', 'hi', 'hey', 'given','instagram','please', 'like', 'kindly']\n",
    "\n",
    "                #Converting to keywords\n",
    "                captions,genuinity_score = process(captions,avoidwords)\n",
    "                caption_array = captions[0]\n",
    "                bio,ignore_this_var = process(bio,avoidwords)\n",
    "                bio_array = bio[0]\n",
    "                # Punishing accounts which has less than 3 words in caption\n",
    "                if len(caption_array) < 5*(total_posts):\n",
    "                    raise Exception(\"Too less words for categorization\")\n",
    "\n",
    "                #Temporary array i-> interim\n",
    "                icaption_array = [z for z in caption_array]\n",
    "                ibio_array = [z for z in bio_array]\n",
    "                # Removing words not in dictionary also single characters\n",
    "                discarded_words = []\n",
    "                discarded_bio_words = []\n",
    "                for x in caption_array:\n",
    "                    try:\n",
    "                        checkword = w.similarity(x,'something') #Check word if exist in googlenews\n",
    "                        if len(x) <2: #Removing single character\n",
    "                            icaption_array.pop(icaption_array.index(x))\n",
    "                    except KeyError:\n",
    "                        discarded_words.append(icaption_array.pop(icaption_array.index(x)))\n",
    "                for x in bio_array:\n",
    "                    try:\n",
    "                        checkword = w.similarity(x,'something') #Check word if exist in googlenews\n",
    "                        if len(x) <2: #Removing single character\n",
    "                            ibio_array.pop(ibio_array.index(x))\n",
    "                    except KeyError:\n",
    "                        discarded_bio_words.append(ibio_array.pop(ibio_array.index(x)))\n",
    "                #Restore Array\n",
    "                caption_array = [z for z in icaption_array]\n",
    "                bio_array = [z for z in ibio_array]\n",
    "                # Check similarity in discarded words\n",
    "                discard_word_scores = [0]*len(categories)\n",
    "                discard_bio_word_scores = [0]*len(categories)\n",
    "                for x_word in discarded_words:\n",
    "                    for category_index in range(len(categories_list)):\n",
    "                        for the_category in categories_list[category_index]:\n",
    "                            if the_category in x_word:\n",
    "                                discard_word_scores[category_index] = 1 + discard_word_scores[category_index]\n",
    "                for x_word in discarded_bio_words:\n",
    "                    for category_index in range(len(categories_list)):\n",
    "                        for the_category in categories_list[category_index]:\n",
    "                            if the_category in x_word:\n",
    "                                discard_bio_word_scores[category_index] = 1 + discard_bio_word_scores[category_index]           \n",
    "                if len(caption_array) ==0:\n",
    "                    raise Exception(\"No Words in profile for categorization or Different language\")\n",
    "\n",
    "                # Word2vec computation\n",
    "                frame = pd.DataFrame()\n",
    "                frame, top_keywords = compute2(caption_array,categories_list,categories,3) ##############\n",
    "                # Word2vec for bio\n",
    "                frame_bio = pd.DataFrame()\n",
    "                bio_score = [0]*len(categories)\n",
    "                for x_word in bio_array:\n",
    "                    for category_index in range(len(categories_list)):\n",
    "                        for the_category in categories_list[category_index]:\n",
    "                            if the_category in x_word:\n",
    "                                bio_score[category_index] = 1 + bio_score[category_index]\n",
    "            \n",
    "                for sc in range(len(bio_score)):\n",
    "                    bio_score[sc] = bio_score[sc] + discard_bio_word_scores[sc] \n",
    "\n",
    "                # Add up computed score with discarded score\n",
    "                score_column = frame['Scores'].tolist()\n",
    "                for ind in range(len(score_column)):\n",
    "                    score_column[ind] = score_column[ind] + discard_word_scores[ind]\n",
    "\n",
    "                # Add weighted score of bio in main score\n",
    "                normalize_bio_score = normalizeSD(bio_score,3)\n",
    "                for sc in range(len(normalize_bio_score)):\n",
    "                    if normalize_bio_score[sc] == 1:\n",
    "                        score_column[sc] = score_column[sc]*2\n",
    "                frame['Scores'] = score_column\n",
    "\n",
    "                #Convert to Percentage\n",
    "                per = frame['Scores'].tolist()\n",
    "                per_sum = sum(per)\n",
    "                for x in range(len(per)):\n",
    "                    temp_number = (float)(per[x])\n",
    "                    per[x] = round((temp_number/per_sum)*100)\n",
    "                frame['Percentage'] = per\n",
    "\n",
    "\n",
    "                #Store profile percentage\n",
    "                row_df = get_row_pscore(col_name,dfnew,i,frame,genuinity_score,top_keywords.tolist(),'Percentage')\n",
    "                profile_percentages = profile_percentages.append(row_df,ignore_index=True)\n",
    "\n",
    "                # POST API Request\n",
    "                file = to_dict_api(frame['Percentage'].tolist(),API_categories,top_keywords,userid,genuinity_score)\n",
    "                url = 'http://44.229.68.155/insta_user/add_category_to_insta_user'\n",
    "                y = requests.post(url, data = file,headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "\n",
    "                if y.status_code !=200:\n",
    "                    raise Exception(\"Post request error {}\".format(y.status_code))\n",
    "                \n",
    "                idsdone = idsdone +1\n",
    "                print(\"ID no. {} Done! Total {} ids done\".format(userid,idsdone))\n",
    "                \n",
    "            except Exception as Argument:\n",
    "                # creating/opening a file \n",
    "                f = open(r\"errorfile.txt\", \"a\") \n",
    "                # writing in the file \n",
    "                f.write(\"Userid\\t\"+str(userid)+\"\\t: \"+str(Argument)+str(\"\\n\")) \n",
    "                # closing the file \n",
    "                f.close()\n",
    "\n",
    "        # END of Main Categorization #\n",
    "        profile_percentages.to_csv(r'profile_backup.csv',mode='a',header=False,index =False)\n",
    "        pages = pages +1\n",
    "        toc = time.time()\n",
    "        print(txt.format(pages,last_id,toc-new_tic))\n",
    "        # Request new page\n",
    "        x = requests.get('http://44.229.68.155/insta_users/get_uncategorized_accounts?limit=10&current_id='+str(last_id), headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "        data = x.json()\n",
    "        df = pd.DataFrame(data['users'])\n",
    "        status = x.status_code\n",
    "\n",
    "    except Exception as Argument:\n",
    "        # creating/opening a file \n",
    "        f = open(r\"errorfile.txt\", \"a\") \n",
    "        # writing in the file \n",
    "        f.write(\"Currently in \"+str(pages)+\"\\t\"+str(Argument)+str(\"\\n\")) \n",
    "        # closing the file \n",
    "        f.close()  \n",
    "\n",
    "toc = time.time()\n",
    "f = open(r\"errorfile.txt\", \"a\") \n",
    "# writing in the file \n",
    "f.write(\"The model ran in \"+str(toc - tic)+\" seconds\"+str(\"\\n\")) \n",
    "f.write(\"Total ids done: \"+str(idsdone)+\"\\n\")\n",
    "f.write(\"Last user_id {}\".format(last_id)+\"\\n\")\n",
    "# closing the file \n",
    "f.close() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(API_categories))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
