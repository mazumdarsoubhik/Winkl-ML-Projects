{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all packages.\n",
      "Loading GoogleNews...\n",
      "Loaded GoogleNews!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import psycopg2\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from itertools import chain \n",
    "import math\n",
    "import time\n",
    "import logging \n",
    "import requests\n",
    "import json\n",
    "print(\"Imported all packages.\")\n",
    "print(\"Loading GoogleNews...\")\n",
    "from gensim import models\n",
    "w = models.KeyedVectors.load_word2vec_format(r\"F:\\Pretrained Models\\GoogleNews-vectors-negative300.bin.gz\", binary=True, limit=2100000)\n",
    "print(\"Loaded GoogleNews!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['food', 'fashion', 'makeup', 'beauty', 'lifestyle','luxury', 'travel','photography','fitness','sports','gaming', 'entertainment', 'technology','investment','education', 'animal', 'health', 'inspiration']\n",
    "categories_list = [['food', 'recipes', 'recipe', 'cooking', 'fried'],\n",
    "              ['fashion', 'outfit', 'clothes', 'menswear','wear'],\n",
    "              ['makeup', 'eyeliner', 'bridal', 'shades', 'airbrush'],\n",
    "              ['beauty', 'pimple', 'skin', 'oil', 'hair'],\n",
    "              ['lifestyle', 'life', 'class', 'style', 'happy'],\n",
    "              ['luxury', 'chic', 'handbag', 'stylish', 'brand'],\n",
    "              ['travel', 'world', 'destination', 'adventure', 'landscapes'],\n",
    "              ['photography', 'photo', 'editing', 'creative', 'artist'],\n",
    "              ['fitness', 'nutrition', 'workout', 'healthy', 'exercise'],\n",
    "              ['sport', 'sports'],\n",
    "              ['gaming', 'gamer', 'fun', 'games', 'stream'],\n",
    "              ['entertainment', 'movies', 'series', 'network', 'comedy'],\n",
    "              ['technology', 'tech', 'geek', 'smartphones', 'mobiles'],\n",
    "              ['investment', 'financial', 'stocks', 'market', 'trade'],\n",
    "              ['education', 'lectures', 'competitive', 'exams', 'coaching'],\n",
    "              ['animal', 'habitat', 'wild', 'documentary', 'nature'],\n",
    "              ['health', 'medical', 'prevent', 'treat', 'heal'],\n",
    "              ['psychology', 'motivation', 'inspire', 'happiness', 'mind','spiritual']]\n",
    "col_name = ['user_id','url','Food','Fashion', 'Makeup', 'Beauty', 'Lifestyle','Luxury', 'Travel','Photography','Fitness','Sports','Gaming', 'Entertainment', 'Gadgets & Tech','Finance','Education', 'Animal/Pet', 'Health', 'Self Improvement','Art', 'Parenting', 'Books', 'genuinity_score','top keywords']\n",
    "API_categories = ['Food','Fashion', 'Makeup', 'Beauty', 'Lifestyle','Luxury', 'Travel','Photography','Fitness','Sports','Gaming', 'Entertainment', 'Gadgets & Tech','Finance','Education', 'Animal/Pet', 'Health','Art', 'Self Improvement', 'Parenting', 'Books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frame pre-processing function\n",
    "def process(array,avoidwords):\n",
    "    hashes = len(re.findall(r'#',str(array))) #counting hashtags\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',str(array))  #Remove Numbers\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text) # Remove nums\n",
    "    text = re.sub(r'\\s+',' ',text)  #Remove extra space\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\",' ',text)  #Remove special characters\n",
    "    text = text.lower()  #Lower case all\n",
    "    text = nltk.sent_tokenize(text)  #Tokenize to sentences \n",
    "    keywords = [nltk.word_tokenize(sentence) for sentence in text]\n",
    "    raw_cap = len(keywords[0]) # Total number of words in caption\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(avoidwords)\n",
    "    for i in range(len(keywords)):\n",
    "        keywords[i] = [word for word in keywords[i] if word not in stop_words]\n",
    "    genuinity_percent = (raw_cap-hashes)*100/raw_cap\n",
    "    return keywords,genuinity_percent\n",
    "\n",
    "\n",
    "# normalize() -> given an array, converts to 1/0, top int(pos) will be 1\n",
    "def normalize(keys, pos =3):  \n",
    "    ax = [i for i in keys]\n",
    "    temp = [i for i in keys]\n",
    "    temp.sort()\n",
    "    temp = temp[-pos:]\n",
    "    for x in temp:\n",
    "        ax[keys.index(x)] = 1\n",
    "    for x in range(len(ax)):\n",
    "        if ax[x] != 1:\n",
    "            ax[x] = 0\n",
    "    return ax\n",
    "\n",
    "def normalizeSD(keys, thre =3):    # Given score array return shortlisted cats in given threshold\n",
    "    ax = deviation(keys)\n",
    "    ax = dev_shortlist(ax,thre)\n",
    "    return ax\n",
    "def deviation(array):\n",
    "    mu = max(array)\n",
    "    l = len(array)\n",
    "    ar = []\n",
    "    for x in range(l):\n",
    "        ar.append(math.sqrt((array[x]-mu)**2)/l)\n",
    "    total = sum(ar)\n",
    "    for x in range(l):\n",
    "        if total != 0:\n",
    "            ar[x] = (ar[x]/total)*100\n",
    "    return ar\n",
    "\n",
    "def mean_deviation(array):\n",
    "    l = len(array)\n",
    "    mu = sum(array)/l\n",
    "    ar = []\n",
    "    for x in range(l):\n",
    "        ar.append(math.sqrt((array[x]-mu)**2)/l)\n",
    "    total = sum(ar)\n",
    "    for x in range(l):\n",
    "        if total != 0:\n",
    "            ar[x] = (ar[x]/total)*100\n",
    "    return ar\n",
    "\n",
    "def dev_shortlist(dev_array,thre = 2):  # Shortlist using threshold from deviation array | return array in 1/0\n",
    "    final_cat = [0]*len(dev_array)\n",
    "    for i in range(len(dev_array)):\n",
    "        if dev_array[i] <=thre:\n",
    "            final_cat[i] = 1\n",
    "    return final_cat\n",
    "\n",
    "def compute1(caption,category,top =3):\n",
    "    ar = []\n",
    "    score = []\n",
    "\n",
    "    # Code to get frequency distribution and unique keywords array\n",
    "    keywords = []\n",
    "    caption_freq = []\n",
    "    counts = Counter(caption)\n",
    "    if len(counts) > 0:\n",
    "        labels, values = zip(*counts.items())\n",
    "        ## sort your values in descending order\n",
    "        indSort = np.argsort(values)[::-1]\n",
    "        ## rearrange your data\n",
    "        keywords = np.array(labels)[indSort]  # Label\n",
    "        caption_freq = np.array(values)[indSort]  # Values\n",
    "    \n",
    "    # Detect words not in Google Dict | Put freq = 0\n",
    "    for x in keywords:\n",
    "        try:\n",
    "            restConst = w.similarity(x,'something')\n",
    "        except KeyError:\n",
    "            caption_freq[np.where(keywords == x)] = 0\n",
    "        \n",
    "    #Google similaity function\n",
    "    for x in category:\n",
    "        empty = []\n",
    "        for y in keywords:\n",
    "            try:\n",
    "                empty.append(w.similarity(x,y))\n",
    "            except:\n",
    "                empty.append(0)\n",
    "        ar.append(empty)\n",
    "    \n",
    "    # Store the similarity values in dataframe\n",
    "    frame = pd.DataFrame()\n",
    "    frame = pd.DataFrame(ar, columns = keywords)\n",
    "  \n",
    "    #Normalize | top select\n",
    "    for key in frame.columns:\n",
    "        frame[key] = normalizeSD(frame[key].tolist(),top)\n",
    "    \n",
    "    # Multiply with frequency\n",
    "    for row in range(len(frame)):\n",
    "        frame.values[row] = [i*j for i,j in zip(frame.values[row],caption_freq)]\n",
    "    # Sum the values => Score\n",
    "    for row in range(len(frame)):\n",
    "        score.append(sum(frame.values[row]))\n",
    "    \n",
    "    frame['category'] = category\n",
    "    frame['Scores'] = score\n",
    "    return frame,keywords[:20]\n",
    "\n",
    "# compute() => category[] to be called outside\n",
    "def compute(caption,category_list,category,top =3):\n",
    "    ar = []\n",
    "    score = []\n",
    "\n",
    "    # Code to get frequency distribution and unique keywords array\n",
    "    keywords = []\n",
    "    caption_freq = []\n",
    "    counts = Counter(caption)\n",
    "    if len(counts) > 0:\n",
    "        labels, values = zip(*counts.items())\n",
    "        ## sort your values in descending order\n",
    "        indSort = np.argsort(values)[::-1]\n",
    "        ## rearrange your data\n",
    "        keywords = np.array(labels)[indSort]  # Label\n",
    "        caption_freq = np.array(values)[indSort]  # Values\n",
    "    \n",
    "    # Detect words not in Google Dict | Put freq = 0\n",
    "    for x in keywords:\n",
    "        try:\n",
    "            restConst = w.similarity(x,'something')\n",
    "        except KeyError:\n",
    "            caption_freq[np.where(keywords == x)] = 0\n",
    "        \n",
    "    #Google similaity function\n",
    "    for c_tag in range(len(category_list)):\n",
    "        empty1 = []\n",
    "        for the_word in keywords:\n",
    "            empty2 = []\n",
    "            for k_tag in range(len(category_list[c_tag])):\n",
    "                try:\n",
    "                    empty2.append(w.similarity(category_list[c_tag][k_tag],the_word))\n",
    "                except:\n",
    "                    empty2.append(0)\n",
    "            empty1.append(sum(empty2)/len(empty2))\n",
    "        ar.append(empty1)\n",
    "    # Store the similarity values in dataframe\n",
    "    frame = pd.DataFrame()\n",
    "    frame = pd.DataFrame(ar, columns = keywords)\n",
    "  \n",
    "    #Normalize | top select\n",
    "    for key in frame.columns:\n",
    "        frame[key] = normalizeSD(frame[key].tolist(),top)\n",
    "    \n",
    "    # Multiply with frequency\n",
    "    for row in range(len(frame)):\n",
    "        frame.values[row] = [i*j for i,j in zip(frame.values[row],caption_freq)]\n",
    "    # Sum the values => Score\n",
    "    for row in range(len(frame)):\n",
    "        score.append(sum(frame.values[row]))\n",
    "    \n",
    "    frame['category'] = category\n",
    "    frame['Scores'] = score\n",
    "    return frame,keywords[:20]\n",
    "\n",
    "def get_row_pscore(col_name,f1,i,f2,genuinity_score,top_keywords, scoreType):  # f1-mainframe | f2-frame\n",
    "    ud = f1.loc[i,'id']\n",
    "    ul = f1.loc[i,'url']\n",
    "    row_in_array = [ud,ul]\n",
    "    score_array = f2[scoreType].tolist()\n",
    "    empty_score = [0]*(len(col_name)-4-len(score_array))\n",
    "    score_array.extend(empty_score)\n",
    "    row_in_array.extend(score_array)\n",
    "    row_in_array.append(genuinity_score)\n",
    "    row_in_array.append(str([top_keywords]))\n",
    "    zip_it = zip(col_name,row_in_array)\n",
    "    convert_to_dict = dict(zip_it)\n",
    "    return convert_to_dict\n",
    "\n",
    "# To make data in \n",
    "# DB format | API post \n",
    "def to_dict_api(percentages,categories,top_keywords,frame,i): #frame and i to get id\n",
    "    mydict = {}\n",
    "    cat_array =[]\n",
    "    empty_percent = [0]*(len(categories)-len(percentages))\n",
    "    percent_array = [y for y in percentages]\n",
    "    percent_array.extend(empty_percent)\n",
    "    mydict['user_id'] = frame.loc[i,'id']\n",
    "    mydict['keywords'] = json.dumps(top_keywords.tolist())\n",
    "    for j in range(len(categories)):\n",
    "        cat_array.append({'tag':categories[j],'percentage':percent_array[j]})\n",
    "    mydict['categories'] = json.dumps(cat_array)\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID no. 15004 Done! Total 0 ids done\n",
      "ID no. 15011 Done! Total 1 ids done\n",
      "ID no. 15017 Done! Total 2 ids done\n",
      "ID no. 15022 Done! Total 3 ids done\n",
      "ID no. 15025 Done! Total 4 ids done\n",
      "ID no. 15026 Done! Total 5 ids done\n",
      "ID no. 15030 Done! Total 6 ids done\n",
      "ID no. 15033 Done! Total 7 ids done\n",
      "ID no. 15041 Done! Total 8 ids done\n",
      "ID no. 15063 Done! Total 9 ids done\n",
      "Done 1 pages, the last_id is 15063 and time taken 40.29780820000042 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>Food</th>\n",
       "      <th>Fashion</th>\n",
       "      <th>Makeup</th>\n",
       "      <th>Beauty</th>\n",
       "      <th>Lifestyle</th>\n",
       "      <th>Luxury</th>\n",
       "      <th>Travel</th>\n",
       "      <th>Photography</th>\n",
       "      <th>...</th>\n",
       "      <th>Finance</th>\n",
       "      <th>Education</th>\n",
       "      <th>Animal/Pet</th>\n",
       "      <th>Health</th>\n",
       "      <th>Self Improvement</th>\n",
       "      <th>Art</th>\n",
       "      <th>Parenting</th>\n",
       "      <th>Books</th>\n",
       "      <th>genuinity_score</th>\n",
       "      <th>top keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15004</td>\n",
       "      <td>https://www.instagram.com/miss__vashisht_/</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73.521916</td>\n",
       "      <td>[array(['skin', 'beauty', 'insta', 'makeup', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15011</td>\n",
       "      <td>https://www.instagram.com/oklastbite/</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.665531</td>\n",
       "      <td>[array(['food', 'pune', 'one', 'blogpost', 'bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15017</td>\n",
       "      <td>https://www.instagram.com/sanjanasar/</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74.766141</td>\n",
       "      <td>[array(['love', 'life', 'photography', 'dance'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15022</td>\n",
       "      <td>https://www.instagram.com/foodstalkker/</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.225090</td>\n",
       "      <td>[array(['food', 'blogger', 'mumbai', 'bahrain'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15025</td>\n",
       "      <td>https://www.instagram.com/heyitsdelhi/</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.799072</td>\n",
       "      <td>[array(['food', 'delhi', 'foodie', 'updates', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15026</td>\n",
       "      <td>https://www.instagram.com/sarangrai/</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.816788</td>\n",
       "      <td>[array(['dance', 'love', 'video', 'dancers', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15030</td>\n",
       "      <td>https://www.instagram.com/imsahilbrown/</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.091483</td>\n",
       "      <td>[array(['inspirational', 'love', 'positivity',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15033</td>\n",
       "      <td>https://www.instagram.com/jaipur__blogger/</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.909411</td>\n",
       "      <td>[array(['jaipur', 'blogger', 'food', 'daily', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15041</td>\n",
       "      <td>https://www.instagram.com/officialgauravkothari/</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.425019</td>\n",
       "      <td>[array(['mumbai', 'india', 'photography', 'sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15063</td>\n",
       "      <td>https://www.instagram.com/ritika_shyam/</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.323907</td>\n",
       "      <td>[array(['chandigarh', 'happy', 'love', 'lifest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                                               url Food Fashion  \\\n",
       "0   15004        https://www.instagram.com/miss__vashisht_/    9       2   \n",
       "1   15011             https://www.instagram.com/oklastbite/   24       1   \n",
       "2   15017             https://www.instagram.com/sanjanasar/   10       2   \n",
       "3   15022           https://www.instagram.com/foodstalkker/   16       1   \n",
       "4   15025            https://www.instagram.com/heyitsdelhi/   37       2   \n",
       "5   15026              https://www.instagram.com/sarangrai/    9       1   \n",
       "6   15030           https://www.instagram.com/imsahilbrown/   18       2   \n",
       "7   15033        https://www.instagram.com/jaipur__blogger/   19       1   \n",
       "8   15041  https://www.instagram.com/officialgauravkothari/   11       2   \n",
       "9   15063           https://www.instagram.com/ritika_shyam/   16       3   \n",
       "\n",
       "  Makeup Beauty Lifestyle Luxury Travel Photography  ... Finance Education  \\\n",
       "0      6      5         3     24      1           4  ...      11         3   \n",
       "1      2      4         4     22      1           3  ...       5         3   \n",
       "2      5      7         5     27      2           2  ...       6         4   \n",
       "3      3      5         5     27      2           3  ...       5         4   \n",
       "4      1      3         4     18      1           2  ...       5         3   \n",
       "5      3      5         5     32      2           2  ...       6         4   \n",
       "6      4      5         5     25      2           3  ...       5         3   \n",
       "7      2      4         4     19      1           3  ...      12         4   \n",
       "8      2      4         5     27      2           2  ...       9         3   \n",
       "9     10      6         5     26      1           1  ...       5         3   \n",
       "\n",
       "  Animal/Pet Health Self Improvement Art Parenting Books genuinity_score  \\\n",
       "0          0      3                5   0         0     0       73.521916   \n",
       "1          1      2                4   0         0     0       81.665531   \n",
       "2          1      4                6   0         0     0       74.766141   \n",
       "3          1      2                4   0         0     0       51.225090   \n",
       "4          0      1                4   0         0     0       47.799072   \n",
       "5          2      3                6   0         0     0       79.816788   \n",
       "6          1      3                4   0         0     0       63.091483   \n",
       "7          0      2                4   0         0     0       87.909411   \n",
       "8          0      1                6   0         0     0       52.425019   \n",
       "9          0      2                5   0         0     0       66.323907   \n",
       "\n",
       "                                        top keywords  \n",
       "0  [array(['skin', 'beauty', 'insta', 'makeup', '...  \n",
       "1  [array(['food', 'pune', 'one', 'blogpost', 'bl...  \n",
       "2  [array(['love', 'life', 'photography', 'dance'...  \n",
       "3  [array(['food', 'blogger', 'mumbai', 'bahrain'...  \n",
       "4  [array(['food', 'delhi', 'foodie', 'updates', ...  \n",
       "5  [array(['dance', 'love', 'video', 'dancers', '...  \n",
       "6  [array(['inspirational', 'love', 'positivity',...  \n",
       "7  [array(['jaipur', 'blogger', 'food', 'daily', ...  \n",
       "8  [array(['mumbai', 'india', 'photography', 'sho...  \n",
       "9  [array(['chandigarh', 'happy', 'love', 'lifest...  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "x = requests.get('http://44.229.68.155/insta_users/get_uncategorized_accounts?limit=10&current_id=15000', headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "status = x.status_code\n",
    "data = x.json()\n",
    "df = pd.DataFrame(data['users'])\n",
    "pages = 0\n",
    "idsdone = 0\n",
    "txt = \"Done {} pages, the last_id is {} and time taken {} seconds\"\n",
    "\n",
    "while(len(data['users']) !=0 and pages<1):\n",
    "    try:\n",
    "        new_tic = time.perf_counter()\n",
    "        if(status != 200):\n",
    "            raise Exception(\"GET request error: {}\".format(status))\n",
    "        dfnew = pd.DataFrame(columns=['id','handle','name','url','gender','country','captions','bio'], data = df[['id','handle','name','url','gender','country','captions','bio']].values)\n",
    "        last_id = dfnew['id'].iloc[-1]\n",
    "        # Fresh dataframe\n",
    "        profile_percentages =  pd.DataFrame(columns = col_name)\n",
    "        \n",
    "        # Main Categorization # \n",
    "        for i in range(len(dfnew)):\n",
    "\n",
    "            try:\n",
    "                #Store userid | caption | total posts\n",
    "                userid = dfnew['id'].iloc[i]\n",
    "                captions = dfnew['captions'].iloc[i]\n",
    "                bio = dfnew['bio'].iloc[i]\n",
    "                total_posts = len(captions)\n",
    "                \n",
    "                # Words which mostly occurs in insta post and we want to avoid considering them for the sake of accuracy of results\n",
    "                avoidwords = ['verified','none','follow','like']\n",
    "\n",
    "                #Converting to keywords\n",
    "                captions,genuinity_score = process(captions,avoidwords)\n",
    "                caption_array = captions[0]\n",
    "                bio,ignore_this_var = process(bio,avoidwords)\n",
    "                bio_array = bio[0]\n",
    "                # Punishing accounts which has less than 3 words in caption\n",
    "                if len(caption_array) < 3*(total_posts):\n",
    "                    raise Exception(\"Too less words for categorization\")\n",
    "\n",
    "                #Temporary array i-> interim\n",
    "                icaption_array = [z for z in caption_array]\n",
    "                ibio_array = [z for z in bio_array]\n",
    "                # Removing words not in dictionary also single characters\n",
    "                discarded_words = []\n",
    "                discarded_bio_words = []\n",
    "                for x in caption_array:\n",
    "                    try:\n",
    "                        checkword = w.similarity(x,'something') #Check word if exist in googlenews\n",
    "                        if len(x) <2: #Removing single character\n",
    "                            icaption_array.pop(icaption_array.index(x))\n",
    "                    except KeyError:\n",
    "                        discarded_words.append(icaption_array.pop(icaption_array.index(x)))\n",
    "                for x in bio_array:\n",
    "                    try:\n",
    "                        checkword = w.similarity(x,'something') #Check word if exist in googlenews\n",
    "                        if len(x) <2: #Removing single character\n",
    "                            ibio_array.pop(ibio_array.index(x))\n",
    "                    except KeyError:\n",
    "                        discarded_bio_words.append(ibio_array.pop(ibio_array.index(x)))\n",
    "                #Restore Array\n",
    "                caption_array = [z for z in icaption_array]\n",
    "                bio_array = [z for z in ibio_array]\n",
    "                # Check similarity in discarded words\n",
    "                discard_word_scores = [0]*len(categories)\n",
    "                discard_bio_word_scores = [0]*len(categories)\n",
    "                for x_word in discarded_words:\n",
    "                    for the_category in categories:\n",
    "                        if the_category in x_word:\n",
    "                            discard_word_scores[categories.index(the_category)] = 1 + discard_word_scores[categories.index(the_category)]\n",
    "                for x_word in discarded_bio_words:\n",
    "                    for the_category in categories:\n",
    "                        if the_category in x_word:\n",
    "                            discard_bio_word_scores[categories.index(the_category)] = 1 + discard_bio_word_scores[categories.index(the_category)]           \n",
    "\n",
    "                if len(caption_array) ==0:\n",
    "                    raise Exception(\"No Words in profile for categorization or Different language\")\n",
    "\n",
    "                # Word2vec computation\n",
    "                frame = pd.DataFrame()\n",
    "                frame, top_keywords = compute(caption_array,categories,categories_list,3)\n",
    "                # Word2vec for bio\n",
    "                frame_bio = pd.DataFrame()\n",
    "                frame_bio,ignore_keywords = compute(bio_array,categories,categories_list,2)\n",
    "                # Storing bio scores\n",
    "                bio_score = frame_bio['Scores'].tolist()\n",
    "                for sc in range(len(bio_score)):\n",
    "                    bio_score[sc] = bio_score[sc] + discard_bio_word_scores[sc] \n",
    "\n",
    "                # Add up computed score with discarded score\n",
    "                score_column = frame['Scores'].tolist()\n",
    "                for ind in range(len(score_column)):\n",
    "                    score_column[ind] = score_column[ind] + discard_word_scores[ind]\n",
    "\n",
    "                # Add weighted score of bio in main score\n",
    "                normalize_bio_score = normalizeSD(bio_score,3)\n",
    "                for sc in range(len(normalize_bio_score)):\n",
    "                    if normalize_bio_score[sc] == 1:\n",
    "                        score_column[sc] = score_column[sc]*2\n",
    "                frame['Scores'] = score_column\n",
    "\n",
    "                #Convert to Percentage\n",
    "                per = frame['Scores'].tolist()\n",
    "                per_sum = sum(per)\n",
    "                for x in range(len(per)):\n",
    "                    temp_number = (float)(per[x])\n",
    "                    per[x] = round((temp_number/per_sum)*100)\n",
    "                frame['Percentage'] = per\n",
    "\n",
    "\n",
    "                #Store profile percentage\n",
    "                row_df_5 = get_row_pscore(col_name,dfnew,i,frame,genuinity_score,top_keywords,'Percentage')\n",
    "                profile_percentages = profile_percentages.append(row_df_5,ignore_index=True)\n",
    "\n",
    "                # POST API Request\n",
    "#                 file = to_dict_api(frame['Percentage'].tolist(),API_categories,top_keywords,dfnew,i)\n",
    "    #             url = 'http://44.229.68.155/insta_user/add_category_to_insta_user'\n",
    "    #             y = requests.post(url, data = file,headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "\n",
    "    #             if y.status_code !=200:\n",
    "    #                 raise Exception(\"Post request error {}\".format(y.status_code))\n",
    "\n",
    "                print(\"ID no. {} Done! Total {} ids done\".format(userid,idsdone))\n",
    "                idsdone = idsdone +1\n",
    "\n",
    "            except Exception as Argument:\n",
    "                # creating/opening a file \n",
    "                f = open(r\"E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\errorfile.txt\", \"a\") \n",
    "                # writing in the file \n",
    "                f.write(\"Userid\\t\"+str(userid)+\"\\t: \"+str(Argument)+str(\"\\n\")) \n",
    "                # closing the file \n",
    "                f.close()  \n",
    "\n",
    "        # END of Main Categorization #\n",
    "\n",
    "        pages = pages +1\n",
    "        profile_percentages.to_csv(r'E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\Testing_book.csv',mode='a',header=False,index =False)\n",
    "#         display(profile_percentages)\n",
    "        toc = time.perf_counter()\n",
    "        print(txt.format(pages,last_id,toc-new_tic))\n",
    "        # Request new page\n",
    "        x = requests.get('http://44.229.68.155/insta_users/get_uncategorized_accounts?limit=10&current_id='+str(last_id), headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "        data = x.json()\n",
    "        df = pd.DataFrame(data['users'])\n",
    "        status = x.status_code\n",
    "    \n",
    "    except Exception as Argument:\n",
    "        # creating/opening a file \n",
    "        f = open(r\"E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\errorfile.txt\", \"a\") \n",
    "        # writing in the file \n",
    "        f.write(\"Currently in \"+str(pages)+\"\\t\"+str(Argument)+str(\"\\n\")) \n",
    "        # closing the file \n",
    "        f.close()  \n",
    "    \n",
    "    \n",
    "\n",
    "toc = time.perf_counter()\n",
    "f = open(r\"E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\errorfile.txt\", \"a\") \n",
    "# writing in the file \n",
    "f.write(\"The model ran in \"+str(toc - tic)+\" seconds\"+str(\"\\n\")) \n",
    "f.write(\"Total ids done: \"+str(idsdone))\n",
    "# closing the file \n",
    "f.close() \n",
    "display(profile_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_percentages.to_csv(r'E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\testing_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function from random accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = pd.read_csv(r'E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\RandomInfluencers.csv')\n",
    "profile_percentages =  pd.DataFrame(columns = col_name)\n",
    "for i in range(len(dfnew)):\n",
    "\n",
    "    try:\n",
    "        #Store userid | caption | total posts\n",
    "        userid = dfnew['id'].iloc[i]\n",
    "        captions = dfnew['captions'].iloc[i]\n",
    "        bio = dfnew['bio'].iloc[i]\n",
    "        total_posts = len(captions)\n",
    "\n",
    "        # Words which mostly occurs in insta post and we want to avoid considering them for the sake of accuracy of results\n",
    "        avoidwords = ['verified','none','follow','like']\n",
    "\n",
    "        #Converting to keywords\n",
    "        captions,genuinity_score = process(captions,avoidwords)\n",
    "        caption_array = captions[0]\n",
    "        bio,ignore_this_var = process(bio,avoidwords)\n",
    "        bio_array = bio[0]\n",
    "        # Punishing accounts which has less than 3 words in caption\n",
    "#         print(len(caption_array),2*(total_posts))\n",
    "#         if len(caption_array) < 2*(total_posts):\n",
    "#             raise Exception(\"Too less words for categorization\")\n",
    "\n",
    "        #Temporary array i-> interim\n",
    "        icaption_array = [z for z in caption_array]\n",
    "        ibio_array = [z for z in bio_array]\n",
    "        # Removing words not in dictionary also single characters\n",
    "        discarded_words = []\n",
    "        discarded_bio_words = []\n",
    "        for x in caption_array:\n",
    "            try:\n",
    "                checkword = w.similarity(x,'something') #Check word if exist in googlenews\n",
    "                if len(x) <2: #Removing single character\n",
    "                    icaption_array.pop(icaption_array.index(x))\n",
    "            except KeyError:\n",
    "                discarded_words.append(icaption_array.pop(icaption_array.index(x)))\n",
    "        for x in bio_array:\n",
    "            try:\n",
    "                checkword = w.similarity(x,'something') #Check word if exist in googlenews\n",
    "                if len(x) <2: #Removing single character\n",
    "                    ibio_array.pop(ibio_array.index(x))\n",
    "            except KeyError:\n",
    "                discarded_bio_words.append(ibio_array.pop(ibio_array.index(x)))\n",
    "        #Restore Array\n",
    "        caption_array = [z for z in icaption_array]\n",
    "        bio_array = [z for z in ibio_array]\n",
    "        # Check similarity in discarded words\n",
    "        discard_word_scores = [0]*len(categories)\n",
    "        discard_bio_word_scores = [0]*len(categories)\n",
    "        for x_word in discarded_words:\n",
    "            for the_category in categories:\n",
    "                if the_category in x_word:\n",
    "                    discard_word_scores[categories.index(the_category)] = 1 + discard_word_scores[categories.index(the_category)]\n",
    "        for x_word in discarded_bio_words:\n",
    "            for the_category in categories:\n",
    "                if the_category in x_word:\n",
    "                    discard_bio_word_scores[categories.index(the_category)] = 1 + discard_bio_word_scores[categories.index(the_category)]           \n",
    "\n",
    "        if len(caption_array) ==0:\n",
    "            raise Exception(\"No Words in profile for categorization or Different language\")\n",
    "\n",
    "        # Word2vec computation\n",
    "        frame = pd.DataFrame()\n",
    "        frame, top_keywords = compute1(caption_array,categories,3) #compute(caption_array,categories,categories_list,3)\n",
    "        # Word2vec for bio\n",
    "        frame_bio = pd.DataFrame()\n",
    "        frame_bio,ignore_keywords = compute1(bio_array,categories,2)\n",
    "        # Storing bio scores\n",
    "        bio_score = frame_bio['Scores'].tolist()\n",
    "        for sc in range(len(bio_score)):\n",
    "            bio_score[sc] = bio_score[sc] + discard_bio_word_scores[sc] \n",
    "\n",
    "        # Add up computed score with discarded score\n",
    "        score_column = frame['Scores'].tolist()\n",
    "        for ind in range(len(score_column)):\n",
    "            score_column[ind] = score_column[ind] + discard_word_scores[ind]\n",
    "\n",
    "        # Add weighted score of bio in main score\n",
    "        normalize_bio_score = normalizeSD(bio_score,3)\n",
    "        for sc in range(len(normalize_bio_score)):\n",
    "            if normalize_bio_score[sc] == 1:\n",
    "                score_column[sc] = score_column[sc]*2\n",
    "        frame['Scores'] = score_column\n",
    "\n",
    "        #Convert to Percentage\n",
    "        per = frame['Scores'].tolist()\n",
    "        per_sum = sum(per)\n",
    "        for x in range(len(per)):\n",
    "            temp_number = (float)(per[x])\n",
    "            per[x] = round((temp_number/per_sum)*100)\n",
    "        frame['Percentage'] = per\n",
    "\n",
    "\n",
    "        #Store profile percentage\n",
    "        row_df_5 = get_row_pscore(col_name,dfnew,i,frame,genuinity_score,top_keywords,'Percentage')\n",
    "        profile_percentages = profile_percentages.append(row_df_5,ignore_index=True)\n",
    "\n",
    "        # POST API Request\n",
    "#                 file = to_dict_api(frame['Percentage'].tolist(),API_categories,top_keywords,dfnew,i)\n",
    "#             url = 'http://44.229.68.155/insta_user/add_category_to_insta_user'\n",
    "#             y = requests.post(url, data = file,headers={'Authorization': 'Token ruor7REQi9KJz6wIQKDXvwtt'})\n",
    "\n",
    "#             if y.status_code !=200:\n",
    "#                 raise Exception(\"Post request error {}\".format(y.status_code))\n",
    "\n",
    "        print(\"ID no. {} Done! Total {} ids done\".format(userid,idsdone))\n",
    "        idsdone = idsdone +1\n",
    "\n",
    "    except Exception as Argument:\n",
    "        # creating/opening a file \n",
    "        f = open(r\"E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\errorfile.txt\", \"a\") \n",
    "        # writing in the file \n",
    "        f.write(\"Userid\\t\"+str(userid)+\"\\t: \"+str(Argument)+str(\"\\n\")) \n",
    "        # closing the file \n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "profile_percentages.to_csv(r\"E:\\Winkl Mains\\Task_4_Recategorisation\\DATA\\Random_account_results_old.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
